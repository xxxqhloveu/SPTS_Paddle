Global:
  use_gpu: true
  epoch_num: 200
  log_smooth_window: 20
  print_batch_step: 10
  save_model_dir: ./output/checkpoint_01
  save_epoch_step: 50
  # evaluation is run every 125 iterations
  eval_batch_step: [ 0,125 ]
  cal_metric_during_train: False
  pretrained_model: #./output/pretrain_models/latest
  checkpoints: ./pretrain/paddle_ic15
              # /data/xiaoqihang/myproject/race/paper/SPTS_paddle/output/checkpoint/latest 
  save_inference_dir:
  use_visualdl: False
  infer_img: 
  save_res_path: 

Architecture:
  name: SPTS
  model_type: det
  algorithm: SPTS
  # Transform:
  Backbone:
    name: ResNet
    layers: 50
  Position: 
    position_embedding: sine
    tfm_hidden_dim: 256
    temperature: 
  Transformer:
    tfm_hidden_dim: 256
    nhead: 8
    num_encoder_layers: 6
    num_decoder_layers: 6
    dim_feedforward: 1024
    dropout: 0
    normalize_before: True
    num_classes: 1100
    return_intermediate_dec: False
    num_bins: 1000
    max_num_text_ins: 60
    padding_index: 1099
    sos_index: 1098
    eos_index: 1097
    recog_pad_index: 1096

  # Neck:
  #   name: SPTS_Neck
  #   # out_channels: 256
  # Head:
  #   name: Neck_Head
  # #   hidden_dim: 256
  # #   out_channels: 7

Loss:
  name: CrossEntropyLoss
  sos_index: 1098
  eos_index: 1097
  recog_pad_index: 1096
  eos_loss_coef: 0.01
  num_classes: 1100
  padding_index: 1099

# Optimizer:
#   name: AdamW
#   beta1: 0.9
#   beta2: 0.999
#   lr:
#     name: LinearDecayLR
#     finetune: False
#     learning_rate: 0.00001 # 0.0005
#     warmup_epochs: 5
#     min_lr: 0.00001
#     warmup_min_lr: 0.0001
#     lr_backbone_ratio: 0.1
#     weight_decay: 0.0001
#     # step_size: 200
#     # gamma: 0.1
#   regularizer:
#     name: 'L2'
#     factor: 0.0005
Optimizer:
  name: AdamW
  beta1: 0.9
  beta2: 0.999
  clip_norm: 10.0
  lr:
    name: Linear
    learning_rate: 0.000000000000000001
    warmup_epoch: 1
  regularizer:
    name: 'L2'
    # factor: 0.000001 # 0.0004  # 使得factor*learning_rate不超过1e-4
# Optimizer:
#   name: Momentum
#   momentum: 0.9
#   lr:
#     name: DecayLearningRate
#     learning_rate: 0.007
#     epochs: 1000
#     factor: 0.9
#     end_lr: 0
#   weight_decay: 0.0001

# PostProcess:
#   name: PSEPostProcess
#   thresh: 0
#   box_thresh: 0.85
#   min_area: 16
#   box_type: quad # 'quad' or 'poly'
#   scale: 1

Metric:
  name: DetMetric
  # main_indicator: hmean

Train:
  dataset:
    name: TextSpottingDataset #SimpleDataSet
    data_dir: /data/xiaoqihang/dataset/icdar2015
    label_file_list:
      - train_images
      - ic15_train.json
    ratio_list: 1. # 0.002
    transforms:
      # - RandomCrop:
      #     min_size_ratio: 0.5
      #     max_size_ratio: 1.0
      #     prob: 1.0
      # - RandomRotate:
      #     max_angle: 30
      #     prob: 0.3
      # - RandomResize:
      #     min_size: [672, 704, 736, 768, 800, 832, 864, 896]
      #     max_size: 1600
      # - RandomDistortion:
      #     brightness: 0.5
      #     contrast: 0.5
      #     saturation: 0.5
      #     hue: 0.5
      #     prob: 0.5
      - ToCHWImage:
      - NormalizeImage:
      # - Normalize:
      - MakeSequence:
          num_bins: 1000
          max_num_text_ins: 60
      # - Padding: # 这里我发现train和test统一最大都是hwc=(720, 1280, 3)
  loader:
    shuffle: False
    drop_last: False
    batch_size_per_card: 1
    num_batch: 1
    num_workers: 0
    use_shared_memory: False

Eval:
  dataset:
    name: TextSpottingDataset 
    data_dir: /data/xiaoqihang/dataset/icdar2015
    label_file_list:
      - test_images
      - ic15_test.json
    ratio_list: 1. # 0.005
    transforms:
      # - RandomCrop:
      #       min_size_ratio: 0.5
      #       max_size_ratio: 1.0
      #       prob: 1.0
      #   - RandomRotate:
      #       max_angle: 30
      #       prob: 0.3
      # - RandomDistortion:
      #     brightness: 0.5
      #     contrast: 0.5
      #     saturation: 0.5
      #     hue: 0.5
      #     prob: 0.5
      # - RandomResize:
      #     min_size: [640, 672, 704, 736, 768, 800, 832, 864, 896]
      #     max_size: 1600
      - ToCHWImage:
      - NormalizeImage:
      # - Normalize:
      - MakeSequence:
          num_bins: 1000
          max_num_text_ins: 60
      # - Padding: # 这里我发现train和test统一最大都是hwc=(720, 1280, 3)
  visualize: False
  rec:
    with_lexicon: true
    lexicon_type: 0
    gt_folder: '/data/xiaoqihang/myproject/race/datasets/gt/gt_ic15'
    lexicon_paths:  [
              '/data/xiaoqihang/dataset/lexicons/ic15/GenericVocabulary_new.txt',
              '/data/xiaoqihang/dataset/lexicons/ic15/ch4_test_vocabulary_new.txt',
              '/data/xiaoqihang/datasets/lexicons/ic15/new_strong_lexicon/new_voc_img_',
          ]
    pair_paths: [
              '/data/xiaoqihang/datasets/lexicons/ic15/GenericVocabulary_pair_list.txt',
              '/data/xiaoqihang/datasets/lexicons/ic15/ch4_test_vocabulary_pair_list.txt',
              '/data/xiaoqihang/datasets/lexicons/ic15/new_strong_lexicon/pair_voc_img_',
          ]
    IS_WORDSPOTTING: False
  loader:
    shuffle: True
    drop_last: False
    batch_size_per_card: 1 # must be 1
    num_batch: 1
    num_workers: 0
